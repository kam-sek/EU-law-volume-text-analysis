{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "\n",
    "\n",
    "# Base URL for the search results\n",
    "base_url = \"https://eur-lex.europa.eu/search.html?DTC=false&SUBDOM_INIT=PRE_ACTS&DB_AUTHOR=commission&DTS_SUBDOM=PRE_ACTS&DB_INTER_CODE_TYPE=OLP&DTS_DOM=EU_LAW&lang=en&type=advanced&date0=ALL%3A01012000%7C28082024&qid=1724861301256\"\n",
    "\n",
    "# Function to generate the URL for a specific page\n",
    "def generate_page_url(page_number):\n",
    "    return f\"{base_url}&page={page_number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the total number of pages\n",
    "def get_total_pages(base_url):\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    try:\n",
    "        driver.get(base_url)\n",
    "        wait = WebDriverWait(driver, 40)\n",
    "        double_right_arrow = wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \".fa.fa-angle-double-right\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", double_right_arrow)\n",
    "        wait.until(EC.visibility_of(double_right_arrow))\n",
    "        \n",
    "        try:\n",
    "            double_right_arrow.click()\n",
    "        except ElementClickInterceptedException:\n",
    "            driver.execute_script(\"arguments[0].click();\", double_right_arrow)\n",
    "\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.ID, 'pagingInput1'))\n",
    "        )\n",
    "        total_pages = int(driver.find_element(By.ID, 'pagingInput1').get_attribute('value'))\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single page and extract data with retry mechanism\n",
    "def process_page(page_number, retries=3):\n",
    "    data_list = []\n",
    "    driver = None\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "            driver.set_window_size(1920, 1080)\n",
    "\n",
    "            page_url = generate_page_url(page_number)\n",
    "            driver.get(page_url)\n",
    "\n",
    "            wait = WebDriverWait(driver, 40)  # Increased timeout to 40 seconds\n",
    "            wait.until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, 'SearchResult'))\n",
    "            )\n",
    "            search_results = driver.find_elements(By.CLASS_NAME, 'SearchResult')\n",
    "\n",
    "            # Print found results\n",
    "            print(f\"Found {len(search_results)} results on page {page_number}\")\n",
    "\n",
    "            for result in search_results:\n",
    "                try:\n",
    "                    more_info_button = result.find_element(By.XPATH, \".//button[contains(@class, 'ViewMoreInfo')]\")\n",
    "                    for child in result.find_elements(By.XPATH, \".//div[contains(@class, 'SearchResultData') and contains(@class, 'collapse') and contains(@class, 'in')]\"):\n",
    "                        driver.execute_script(\"arguments[0].classList.remove('collapse')\", child)\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "\n",
    "                data = {}\n",
    "                try:\n",
    "                    title_element = result.find_element(By.TAG_NAME, 'a')\n",
    "                    data['Title'] = title_element.text\n",
    "                    data['Link'] = title_element.get_attribute('href')\n",
    "                except NoSuchElementException:\n",
    "                    data['Title'] = None\n",
    "                    data['Link'] = None\n",
    "\n",
    "                try:\n",
    "                    data['CELEX'] = result.find_element(By.XPATH, \".//dt[normalize-space(text())='CELEX number:']/following-sibling::dd[1]\").text\n",
    "                except NoSuchElementException:\n",
    "                    data['CELEX'] = None\n",
    "\n",
    "                try:\n",
    "                    data['Form'] = result.find_element(By.XPATH, \".//dt[contains(text(), 'Form')]/following-sibling::dd[1]\").text\n",
    "                except NoSuchElementException:\n",
    "                    data['Form'] = None\n",
    "\n",
    "                try:\n",
    "                    data['Author'] = result.find_element(By.XPATH, \".//dt[contains(text(), 'Author')]/following-sibling::dd[1]\").text\n",
    "                except NoSuchElementException:\n",
    "                    data['Author'] = None\n",
    "\n",
    "                try:\n",
    "                    data['Date'] = result.find_element(By.XPATH, \".//dt[contains(text(), 'Date of document')]/following-sibling::dd[1]\").text\n",
    "                except NoSuchElementException:\n",
    "                    data['Date'] = None\n",
    "\n",
    "                try:\n",
    "                    data['Pages'] = result.find_element(By.XPATH, \".//dt[contains(text(), 'Number of pages')]/following-sibling::dd[1]\").text\n",
    "                except NoSuchElementException:\n",
    "                    data['Pages'] = None\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "            break  # Exit loop if successful\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            print(f\"TimeoutException on page {page_number}, attempt {attempt + 1}: {str(e)}\")\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Giving up on page {page_number} after {retries} attempts\")\n",
    "        \n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    return page_number, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process multiple pages in parallel\n",
    "def process_pages_in_parallel(start_page, end_page):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_page = {executor.submit(process_page, page): page for page in range(start_page, end_page + 1)}\n",
    "        all_data = []\n",
    "        for future in concurrent.futures.as_completed(future_to_page):\n",
    "            try:\n",
    "                page_number, page_data = future.result()\n",
    "                all_data.append((page_number, page_data))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception during processing page: {str(e)}\")\n",
    "    \n",
    "    all_data.sort(key=lambda x: x[0])\n",
    "    ordered_data = [item for _, sublist in all_data for item in sublist]\n",
    "    return ordered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of pages\n",
    "total_pages = get_total_pages(base_url)\n",
    "#total_pages = 10\n",
    "print(f\"Total number of pages: {total_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first n pages\n",
    "data_list = process_pages_in_parallel(1, total_pages)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the DataFrame\n",
    "eurlex_data_scrapped = df\n",
    "\n",
    "# Save the DataFrame to a CSV file in the ../Data directory\n",
    "eurlex_data_scrapped.to_csv('../Data/eurlex-data-scrapped.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
